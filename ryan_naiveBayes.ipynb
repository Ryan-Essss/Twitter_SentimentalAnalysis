{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aashitarakesh/Twitter_Analysis/blob/main/naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SreK9tZ1Wnrf",
    "outputId": "0be4e81a-44c2-4ccb-c922-47a01c6b633f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [Working]\r",
      "            \r",
      "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
      "\r",
      "0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to cloud.r-pr\r",
      "0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.88.152)\r",
      "                                                                               \r",
      "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
      "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
      "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
      "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
      "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
      "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.2'\n",
    "spark_version = 'spark-3.0.2'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3go-2DKcb97"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"NaiveBayes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLSoU87ply94"
   },
   "outputs": [],
   "source": [
    "# # loads data from google drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "\n",
    "# pd.read_csv('/content/gdrive/My Drive/UPenn Data Analytics/UPenn Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faQ99lqNceP3"
   },
   "outputs": [],
   "source": [
    "# # import data from local system\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bidQpthuWzK8"
   },
   "outputs": [],
   "source": [
    "# # import file from github\n",
    "\n",
    "# from pyspark import SparkFiles\n",
    "\n",
    "# url = 'https://raw.githubusercontent.com/aashitarakesh/Twitter_Analysis/main/sentiment_twitter_clean.csv'\n",
    "# spark.sparkContext.addFile(url)\n",
    "# df = spark.read.csv(SparkFiles.get(\"sentiment_twitter_clean.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# # Show DataFrame\n",
    "# df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mJZ5CsRfASh",
    "outputId": "62e3f4fe-77de-4159-e784-5805bbd320e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+\n",
      "|Index|              Tweets|Label|\n",
      "+-----+--------------------+-----+\n",
      "|  106|real good moment ...|    0|\n",
      "|  217|       reading manga|    0|\n",
      "|  288|need send em acco...|    0|\n",
      "|  540|add myspace myspa...|    0|\n",
      "|  624|sleepy good times...|    0|\n",
      "|  701|nbn someone alrea...|    0|\n",
      "|  808|23 24ï½c possible...|    0|\n",
      "| 1193|nite twitterville...|    0|\n",
      "| 1324|night darlin swee...|    0|\n",
      "| 1332|good morning ever...|    0|\n",
      "| 1368|finally created w...|    0|\n",
      "| 1578|kisha cnt get u t...|    0|\n",
      "| 1595|yes remember band...|    0|\n",
      "| 1861|really love refle...|    0|\n",
      "| 1889|ooo fantasy like ...|    0|\n",
      "| 1899|probs sell nothin...|    0|\n",
      "| 1919|quotnokla connect...|    0|\n",
      "| 1992|stayed late start...|    0|\n",
      "| 2097|read new job cong...|    0|\n",
      "| 2126|havent able sleep...|    0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import file from github\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/aashitarakesh/Twitter_Analysis/main/sentiment_twitter_clean.csv'\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"sentiment_twitter_clean.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1h6nwr4qXkT1"
   },
   "outputs": [],
   "source": [
    "# # Drop all rows with missing information\n",
    "# df = df.dropna(how='any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3d-76d9jbfd",
    "outputId": "db17a5ec-7501-4349-b8a9-2833185b06dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Index: string, Tweets: string, Label: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Verify dropped rows\n",
    "# df.count()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5eeNcfNXnti",
    "outputId": "c17ac8d0-18d3-4b48-9b5f-2a88daafa398"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+------+\n",
      "|Index|              Tweets|Label|length|\n",
      "+-----+--------------------+-----+------+\n",
      "|  106|real good moment ...|    0|    34|\n",
      "|  217|       reading manga|    0|    13|\n",
      "|  288|need send em acco...|    0|    92|\n",
      "|  540|add myspace myspa...|    0|    33|\n",
      "|  624|sleepy good times...|    0|    32|\n",
      "|  701|nbn someone alrea...|    0|    54|\n",
      "|  808|23 24ï½c possible...|    0|    28|\n",
      "| 1193|nite twitterville...|    0|    30|\n",
      "| 1324|night darlin swee...|    0|    25|\n",
      "| 1332|good morning ever...|    0|    22|\n",
      "| 1368|finally created w...|    0|    75|\n",
      "| 1578|kisha cnt get u t...|    0|    45|\n",
      "| 1595|yes remember band...|    0|    38|\n",
      "| 1861|really love refle...|    0|    31|\n",
      "| 1889|ooo fantasy like ...|    0|    37|\n",
      "| 1899|probs sell nothin...|    0|    50|\n",
      "| 1919|quotnokla connect...|    0|    31|\n",
      "| 1992|stayed late start...|    0|    43|\n",
      "| 2097|read new job cong...|    0|    44|\n",
      "| 2126|havent able sleep...|    0|    51|\n",
      "+-----+--------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "# Create a length column to be used as a future feature \n",
    "data_df = df.withColumn('length', length(df['Tweets']))\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0txfyoqdX32S"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "# Create all the features to the data set\n",
    "pos_neg_to_num = StringIndexer(inputCol='Label',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"Tweets\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLT1ASpTc7ST"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8W3vFJ6oZRcV"
   },
   "outputs": [],
   "source": [
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jzk8DNxmZbtU"
   },
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(data_df)\n",
    "cleaned = cleaner.transform(data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSzKdXP0ZdMz",
    "outputId": "2b3d34b4-c551-4852-fd21-061f7f4e797b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(262145,[76764,11...|\n",
      "|  0.0|(262145,[41314,12...|\n",
      "|  0.0|(262145,[942,2032...|\n",
      "|  0.0|(262145,[48063,98...|\n",
      "|  0.0|(262145,[73853,11...|\n",
      "|  0.0|(262145,[2306,178...|\n",
      "|  0.0|(262145,[22346,44...|\n",
      "|  0.0|(262145,[62166,93...|\n",
      "|  0.0|(262145,[88594,15...|\n",
      "|  0.0|(262145,[113432,1...|\n",
      "|  0.0|(262145,[2306,762...|\n",
      "|  0.0|(262145,[18176,51...|\n",
      "|  0.0|(262145,[14118,42...|\n",
      "|  0.0|(262145,[186480,2...|\n",
      "|  0.0|(262145,[13823,12...|\n",
      "|  0.0|(262145,[14118,16...|\n",
      "|  0.0|(262145,[93523,10...|\n",
      "|  0.0|(262145,[16004,68...|\n",
      "|  0.0|(262145,[10345,53...|\n",
      "|  0.0|(262145,[9657,506...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show label and resulting features\n",
    "cleaned.select(['label', 'features']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3N8lITZRtL9",
    "outputId": "9cec70ff-ea31-4898-b78b-1ded8278cba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Index: string (nullable = true)\n",
      " |-- Tweets: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- token_text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stop_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- hash_token: vector (nullable = true)\n",
      " |-- idf_token: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLJlMsDBZlvL",
    "outputId": "1160c3db-6c84-413e-85ce-62451a0697e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Index|              Tweets|label|length|          token_text|         stop_tokens|          hash_token|           idf_token|            features|\n",
      "+-----+--------------------+-----+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  106|real good moment ...|  0.0|    34|[real, good, mome...|[real, good, mome...|(262144,[76764,11...|(262144,[76764,11...|(262145,[76764,11...|\n",
      "|  217|       reading manga|  0.0|    13|    [reading, manga]|    [reading, manga]|(262144,[41314,12...|(262144,[41314,12...|(262145,[41314,12...|\n",
      "|  288|need send em acco...|  0.0|    92|[need, send, em, ...|[need, send, em, ...|(262144,[942,2032...|(262144,[942,2032...|(262145,[942,2032...|\n",
      "|  540|add myspace myspa...|  0.0|    33|[add, myspace, my...|[add, myspace, my...|(262144,[48063,98...|(262144,[48063,98...|(262145,[48063,98...|\n",
      "|  624|sleepy good times...|  0.0|    32|[sleepy, good, ti...|[sleepy, good, ti...|(262144,[73853,11...|(262144,[73853,11...|(262145,[73853,11...|\n",
      "|  701|nbn someone alrea...|  0.0|    54|[nbn, someone, al...|[nbn, someone, al...|(262144,[2306,178...|(262144,[2306,178...|(262145,[2306,178...|\n",
      "|  808|23 24ï½c possible...|  0.0|    28|[23, 24ï½c, possi...|[23, 24ï½c, possi...|(262144,[22346,44...|(262144,[22346,44...|(262145,[22346,44...|\n",
      "| 1193|nite twitterville...|  0.0|    30|[nite, twittervil...|[nite, twittervil...|(262144,[62166,93...|(262144,[62166,93...|(262145,[62166,93...|\n",
      "| 1324|night darlin swee...|  0.0|    25|[night, darlin, s...|[night, darlin, s...|(262144,[88594,15...|(262144,[88594,15...|(262145,[88594,15...|\n",
      "| 1332|good morning ever...|  0.0|    22|[good, morning, e...|[good, morning, e...|(262144,[113432,1...|(262144,[113432,1...|(262145,[113432,1...|\n",
      "| 1368|finally created w...|  0.0|    75|[finally, created...|[finally, created...|(262144,[2306,762...|(262144,[2306,762...|(262145,[2306,762...|\n",
      "| 1578|kisha cnt get u t...|  0.0|    45|[kisha, cnt, get,...|[kisha, cnt, get,...|(262144,[18176,51...|(262144,[18176,51...|(262145,[18176,51...|\n",
      "| 1595|yes remember band...|  0.0|    38|[yes, remember, b...|[yes, remember, b...|(262144,[14118,42...|(262144,[14118,42...|(262145,[14118,42...|\n",
      "| 1861|really love refle...|  0.0|    31|[really, love, re...|[really, love, re...|(262144,[186480,2...|(262144,[186480,2...|(262145,[186480,2...|\n",
      "| 1889|ooo fantasy like ...|  0.0|    37|[ooo, fantasy, li...|[ooo, fantasy, li...|(262144,[13823,12...|(262144,[13823,12...|(262145,[13823,12...|\n",
      "| 1899|probs sell nothin...|  0.0|    50|[probs, sell, not...|[probs, sell, not...|(262144,[14118,16...|(262144,[14118,16...|(262145,[14118,16...|\n",
      "| 1919|quotnokla connect...|  0.0|    31|[quotnokla, conne...|[quotnokla, conne...|(262144,[93523,10...|(262144,[93523,10...|(262145,[93523,10...|\n",
      "| 1992|stayed late start...|  0.0|    43|[stayed, late, st...|[stayed, late, st...|(262144,[16004,68...|(262144,[16004,68...|(262145,[16004,68...|\n",
      "| 2097|read new job cong...|  0.0|    44|[read, new, job, ...|[read, new, job, ...|(262144,[10345,53...|(262144,[10345,53...|(262145,[10345,53...|\n",
      "| 2126|havent able sleep...|  0.0|    51|[havent, able, sl...|[havent, able, sl...|(262144,[9657,506...|(262144,[9657,506...|(262145,[9657,506...|\n",
      "+-----+--------------------+-----+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10283"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.show()\n",
    "cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qew9N86yNBOq"
   },
   "outputs": [],
   "source": [
    "count = cleaned.count()\n",
    "\n",
    "# export dataframe to a csv\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Enable Arrow-based columnar data transfers\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# pandas_df = cleaned.select(\"*\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bazojmacJj-"
   },
   "outputs": [],
   "source": [
    "# pandas_df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FU6wLPJ5cBe_"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Break data down into a training set and a testing set\n",
    "training, testing = cleaned.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqmQoD5lCGga",
    "outputId": "2961fe08-44ec-4909-fe69-9da023aa00dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| Index|              Tweets|label|length|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
      "+------+--------------------+-----+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|100077|       dyededed hair|  0.0|    13|    [dyededed, hair]|    [dyededed, hair]|(262144,[126768,1...|(262144,[126768,1...|(262145,[126768,1...|[-174.52756472984...|[0.99942235398681...|       0.0|\n",
      "|101065|good morning haha...|  0.0|    79|[good, morning, h...|[good, morning, h...|(262144,[24657,53...|(262144,[24657,53...|(262145,[24657,53...|[-833.54066304901...|[1.0,4.4906118047...|       0.0|\n",
      "|101271|goodnight cuppyca...|  0.0|    53|[goodnight, cuppy...|[goodnight, cuppy...|(262144,[60060,61...|(262144,[60060,61...|(262145,[60060,61...|[-534.94019404077...|[1.0,2.6802170049...|       0.0|\n",
      "|101419|          get dinner|  0.0|    10|       [get, dinner]|       [get, dinner]|(262144,[150992,2...|(262144,[150992,2...|(262145,[150992,2...|[-76.717696466503...|[0.99999999942484...|       0.0|\n",
      "|101902|      well good luck|  0.0|    14|  [well, good, luck]|  [well, good, luck]|(262144,[64881,11...|(262144,[64881,11...|(262145,[64881,11...|[-102.83860673236...|[0.99999999993247...|       0.0|\n",
      "|102221|      hahahahaha day|  0.0|    14|   [hahahahaha, day]|   [hahahahaha, day]|(262144,[241560,2...|(262144,[241560,2...|(262145,[241560,2...|[-115.62797214809...|[0.99354903363684...|       0.0|\n",
      "|102307|    bank holiday day|  0.0|    16|[bank, holiday, day]|[bank, holiday, day]|(262144,[146390,2...|(262144,[146390,2...|(262145,[146390,2...|[-166.42276809198...|[1.0,1.3239715782...|       0.0|\n",
      "|102367|download movie qu...|  0.0|    43|[download, movie,...|[download, movie,...|(262144,[13465,48...|(262144,[13465,48...|(262145,[13465,48...|[-466.54765812632...|[1.0,1.5917725789...|       0.0|\n",
      "|102694|musicmonday happy...|  0.0|    49|[musicmonday, hap...|[musicmonday, hap...|(262144,[75038,81...|(262144,[75038,81...|(262145,[75038,81...|[-488.15939979478...|[0.99999999999514...|       0.0|\n",
      "|102791|shameless pluggin...|  0.0|    43|[shameless, plugg...|[shameless, plugg...|(262144,[91665,11...|(262144,[91665,11...|(262145,[91665,11...|[-552.58026885623...|[0.99999999999910...|       0.0|\n",
      "|102907|working framework...|  0.0|   105|[working, framewo...|[working, framewo...|(262144,[52879,69...|(262144,[52879,69...|(262145,[52879,69...|[-1277.0854360121...|[1.0,1.9808254485...|       0.0|\n",
      "|103268| stephen king fo sho|  0.0|    19|[stephen, king, f...|[stephen, king, f...|(262144,[109906,1...|(262144,[109906,1...|(262145,[109906,1...|[-395.13585405750...|[0.99999999999999...|       0.0|\n",
      "|103387|britney spears so...|  0.0|    28|[britney, spears,...|[britney, spears,...|(262144,[1206,694...|(262144,[1206,694...|(262145,[1206,694...|[-332.37659212668...|[1.0,9.3756005833...|       0.0|\n",
      "|103728|mom likes milows ...|  0.0|    62|[mom, likes, milo...|[mom, likes, milo...|(262144,[24346,27...|(262144,[24346,27...|(262145,[24346,27...|[-716.85372008246...|[0.99999999999996...|       0.0|\n",
      "|103972|thanks elephants ...|  0.0|    79|[thanks, elephant...|[thanks, elephant...|(262144,[1696,534...|(262144,[1696,534...|(262145,[1696,534...|[-944.20825713564...|[1.0,1.5940341674...|       0.0|\n",
      "|104344|okay well dont fe...|  0.0|    28|[okay, well, dont...|[okay, well, dont...|(262144,[61899,76...|(262144,[61899,76...|(262145,[61899,76...|[-245.68133140944...|[0.99999917567567...|       0.0|\n",
      "|104714|     surfing twitter|  0.0|    15|  [surfing, twitter]|  [surfing, twitter]|(262144,[1512,459...|(262144,[1512,459...|(262145,[1512,459...|[-137.33642930573...|[0.99999999999061...|       0.0|\n",
      "|104998|back lovely land ...|  0.0|    22|[back, lovely, la...|[back, lovely, la...|(262144,[51218,93...|(262144,[51218,93...|(262145,[51218,93...|[-233.87358435365...|[1.0,2.9956578510...|       0.0|\n",
      "|105854|id like see pictu...|  0.0|    76|[id, like, see, p...|[id, like, see, p...|(262144,[8538,186...|(262144,[8538,186...|(262145,[8538,186...|[-757.23298903331...|[1.0,3.1553782860...|       0.0|\n",
      "|106175|happy birthday sn...|  0.0|    59|[happy, birthday,...|[happy, birthday,...|(262144,[16226,59...|(262144,[16226,59...|(262145,[16226,59...|[-477.80235328125...|[1.0,4.5125275493...|       0.0|\n",
      "+------+--------------------+-----+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tranform the model with the testing data\n",
    "test_results = predictor.transform(testing)\n",
    "test_results.show()\n",
    "# test_results.show(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAbGkWYoC_-f",
    "outputId": "473b7731-21ab-4d70-9d7f-09bab02f04ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting reviews was: 0.907054\n"
     ]
    }
   ],
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "08Nl-J7gDB2o",
    "outputId": "04cf7b22-9304-4d16-cddf-c82047e1d299"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:88: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# export dataframe to a csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "results = test_results.select(\"*\").toPandas()\n",
    "\n",
    "results.to_csv('nb_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLXIaOYbPzKp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO1VR0GrIqnCRM356bcT6Fi",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
